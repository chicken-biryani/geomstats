{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\nApply Expectation Maximization on manifolds and plots the results.<br>\n", "Random data is generated in separate regions of the<br>\n", "manifold. Then Expectation Maximization deduces a Gaussian Mixture Model<br>\n", "that best fits the random data. For the moment<br>\n", "the example works on the Poincar\u00c3\u00a9 Ball hyperbolic space.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import mpl_toolkits.mplot3d.art3d as art3d\n", "from matplotlib.patches import Circle"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import geomstats.backend as gs\n", "from geomstats.geometry.poincare_ball import PoincareBall\n", "from geomstats.learning.expectation_maximization import RiemannianEM, weighted_gmm_pdf"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["DEFAULT_PLOT_PRECISION = 100"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_gaussian_mixture_distribution(\n", "    data,\n", "    mixture_coefficients,\n", "    means,\n", "    variances,\n", "    plot_precision=DEFAULT_PLOT_PRECISION,\n", "    save_path=\"\",\n", "    metric=None,\n", "):\n", "    \"\"\"Plot Gaussian Mixture Model.\"\"\"\n", "    x_axis_samples = gs.linspace(-1, 1, plot_precision)\n", "    y_axis_samples = gs.linspace(-1, 1, plot_precision)\n", "    x_axis_samples, y_axis_samples = gs.meshgrid(x_axis_samples, y_axis_samples)\n", "    z_axis_samples = gs.zeros((plot_precision, plot_precision))\n", "    for z_index, _ in enumerate(z_axis_samples):\n", "        x_y_plane_mesh = gs.concatenate(\n", "            (\n", "                gs.expand_dims(x_axis_samples[z_index], -1),\n", "                gs.expand_dims(y_axis_samples[z_index], -1),\n", "            ),\n", "            axis=-1,\n", "        )\n", "        mesh_probabilities = weighted_gmm_pdf(\n", "            mixture_coefficients, x_y_plane_mesh, means, variances, metric\n", "        )\n", "        z_axis_samples[z_index] = mesh_probabilities.sum(-1)\n", "    fig = plt.figure(\n", "        \"Learned Gaussian Mixture Model \"\n", "        \"via Expectation Maximization on Poincar\u00c3\u00a9 Disc\"\n", "    )\n", "    ax = fig.gca(projection=\"3d\")\n", "    ax.plot_surface(\n", "        x_axis_samples,\n", "        y_axis_samples,\n", "        z_axis_samples,\n", "        rstride=1,\n", "        cstride=1,\n", "        linewidth=1,\n", "        antialiased=True,\n", "        cmap=plt.get_cmap(\"viridis\"),\n", "    )\n", "    z_circle = -0.8\n", "    p = Circle((0, 0), 1, edgecolor=\"b\", lw=1, facecolor=\"none\")\n", "    ax.add_patch(p)\n", "    art3d.pathpatch_2d_to_3d(p, z=z_circle, zdir=\"z\")\n", "    for data_index, _ in enumerate(data):\n", "        ax.scatter(\n", "            data[data_index][0], data[data_index][1], z_circle, c=\"b\", marker=\".\"\n", "        )\n", "    for means_index, _ in enumerate(means):\n", "        ax.scatter(\n", "            means[means_index][0], means[means_index][1], z_circle, c=\"r\", marker=\"D\"\n", "        )\n", "    ax.set_xlim(-1.2, 1.2)\n", "    ax.set_ylim(-1.2, 1.2)\n", "    ax.set_zlim(-0.8, 0.4)\n", "    ax.set_xlabel(\"X\")\n", "    ax.set_ylabel(\"Y\")\n", "    ax.set_zlabel(\"P\")\n", "    plt.savefig(save_path, format=\"pdf\")\n", "    return plt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def expectation_maximisation_poincare_ball():\n", "    \"\"\"Apply EM algorithm on three random data clusters.\"\"\"\n", "    dim = 2\n", "    n_samples = 5\n", "    cluster_1 = gs.random.uniform(low=0.2, high=0.6, size=(n_samples, dim))\n", "    cluster_2 = gs.random.uniform(low=-0.6, high=-0.2, size=(n_samples, dim))\n", "    cluster_3 = gs.random.uniform(low=-0.3, high=0, size=(n_samples, dim))\n", "    cluster_3[:, 0] = -cluster_3[:, 0]\n", "    data = gs.concatenate((cluster_1, cluster_2, cluster_3), axis=0)\n", "    n_clusters = 3\n", "    manifold = PoincareBall(dim=2)\n", "    metric = manifold.metric\n", "    EM = RiemannianEM(\n", "        n_gaussians=n_clusters, metric=metric, initialisation_method=\"random\"\n", "    )\n", "    means, variances, mixture_coefficients = EM.fit(data=data)\n\n", "    # Plot result\n", "    plot = plot_gaussian_mixture_distribution(\n", "        data,\n", "        mixture_coefficients,\n", "        means,\n", "        variances,\n", "        plot_precision=100,\n", "        save_path=\"result.png\",\n", "        metric=metric,\n", "    )\n", "    return plot"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main():\n", "    \"\"\"Apply Expectation Maximization on random data.\n", "    Fits three randomly generated clusters into a\n", "    Gaussian Mixture Model on Poincar\u00c3\u00a9 Ball.\n", "    Then a plot function computes the probability density\n", "    function of the GMM for visualization.\n", "    \"\"\"\n", "    plots = expectation_maximisation_poincare_ball()\n", "    plots.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    if os.environ[\"GEOMSTATS_BACKEND\"] != \"numpy\":\n", "        print(\n", "            \"Expectation Maximization example\\n\"\n", "            \"works with\\n\"\n", "            \"numpy backend.\\n\"\n", "            \"To change backend, write: \"\n", "            \"export GEOMSTATS_BACKEND = 'numpy'.\"\n", "        )\n", "    else:\n", "        main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}